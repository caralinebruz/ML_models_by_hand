{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2005363",
   "metadata": {},
   "source": [
    "# About this implementation\n",
    "\n",
    "Uses BCELoss\n",
    "\n",
    "$y_n$ is the example's label. $\\hat{y}_n$ is the prediction probability (the posterior)\n",
    "\n",
    "\n",
    "$$ \\ell_n = -w_n [y_n \\cdot log \\; \\hat{y}_n + (1-y_n) \\cdot log(1- \\hat{y}_n)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adbde9",
   "metadata": {},
   "source": [
    "## Step 1: Import dataset and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d22fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35b9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b21adef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb039d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test train\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ab66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data myself, rather than with sklearn, for practice\n",
    "\n",
    "mu = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mu) / std\n",
    "X_test = (X_test - mu) / std     # fit it to the same distribution params gathered from train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a02261",
   "metadata": {},
   "source": [
    "### Convert to pytorch tensors (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53351856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f8ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d313755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor dataset for convenience\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "# also create a dataloader to nicely load your tensor data into the epochs\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size=24, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673a869",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20c26d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d5b338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, num_hidden, num_classes):    # calls init function of nn.module\n",
    "        \n",
    "        super(MyMLP, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # one hidden layer\n",
    "        self.linear_combination_1 = torch.nn.Linear(num_features, num_hidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # output layer\n",
    "        self.output_layer = torch.nn.Linear(num_hidden, self.num_classes)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward pass flow\n",
    "            \n",
    "        out = self.linear_combination_1(x)\n",
    "        out = self.relu(out)\n",
    "            \n",
    "        out = self.output_layer(out)\n",
    "        out = self.sigmoid(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3c32d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7618da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model properties and initialize\n",
    "\n",
    "# for the binary classification, either cancer/not cancer. one class is being detected\n",
    "# if we did 2 classes we would need a softmax for the presence of two classes, which we could also do.\n",
    "\n",
    "model = MyMLP(num_features=num_features, num_hidden=1000, num_classes=1)\n",
    "\n",
    "# BCE loss is most stable and most abstracted as well (compare to nll etc.)\n",
    "loss_criterion = torch.nn.BCELoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # i guess, try 0.001 for now.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcb1c4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3adcd000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01/50 | Batch 00/19 | Cost: 0.6571\n",
      "Epoch: 01/50 | Batch 04/19 | Cost: 0.6667\n",
      "Epoch: 01/50 | Batch 08/19 | Cost: 0.6266\n",
      "Epoch: 01/50 | Batch 12/19 | Cost: 0.6385\n",
      "Epoch: 01/50 | Batch 16/19 | Cost: 0.6832\n",
      "Epoch: 02/50 | Batch 00/19 | Cost: 0.6800\n",
      "Epoch: 02/50 | Batch 04/19 | Cost: 0.6543\n",
      "Epoch: 02/50 | Batch 08/19 | Cost: 0.6598\n",
      "Epoch: 02/50 | Batch 12/19 | Cost: 0.6561\n",
      "Epoch: 02/50 | Batch 16/19 | Cost: 0.6596\n",
      "Epoch: 03/50 | Batch 00/19 | Cost: 0.6358\n",
      "Epoch: 03/50 | Batch 04/19 | Cost: 0.6458\n",
      "Epoch: 03/50 | Batch 08/19 | Cost: 0.6119\n",
      "Epoch: 03/50 | Batch 12/19 | Cost: 0.6550\n",
      "Epoch: 03/50 | Batch 16/19 | Cost: 0.6286\n",
      "Epoch: 04/50 | Batch 00/19 | Cost: 0.6108\n",
      "Epoch: 04/50 | Batch 04/19 | Cost: 0.6223\n",
      "Epoch: 04/50 | Batch 08/19 | Cost: 0.6293\n",
      "Epoch: 04/50 | Batch 12/19 | Cost: 0.6133\n",
      "Epoch: 04/50 | Batch 16/19 | Cost: 0.6649\n",
      "Epoch: 05/50 | Batch 00/19 | Cost: 0.6382\n",
      "Epoch: 05/50 | Batch 04/19 | Cost: 0.6184\n",
      "Epoch: 05/50 | Batch 08/19 | Cost: 0.6308\n",
      "Epoch: 05/50 | Batch 12/19 | Cost: 0.6313\n",
      "Epoch: 05/50 | Batch 16/19 | Cost: 0.6586\n",
      "Epoch: 06/50 | Batch 00/19 | Cost: 0.5897\n",
      "Epoch: 06/50 | Batch 04/19 | Cost: 0.6405\n",
      "Epoch: 06/50 | Batch 08/19 | Cost: 0.6345\n",
      "Epoch: 06/50 | Batch 12/19 | Cost: 0.6042\n",
      "Epoch: 06/50 | Batch 16/19 | Cost: 0.6251\n",
      "Epoch: 07/50 | Batch 00/19 | Cost: 0.6128\n",
      "Epoch: 07/50 | Batch 04/19 | Cost: 0.5967\n",
      "Epoch: 07/50 | Batch 08/19 | Cost: 0.6417\n",
      "Epoch: 07/50 | Batch 12/19 | Cost: 0.6153\n",
      "Epoch: 07/50 | Batch 16/19 | Cost: 0.6079\n",
      "Epoch: 08/50 | Batch 00/19 | Cost: 0.6206\n",
      "Epoch: 08/50 | Batch 04/19 | Cost: 0.5733\n",
      "Epoch: 08/50 | Batch 08/19 | Cost: 0.6070\n",
      "Epoch: 08/50 | Batch 12/19 | Cost: 0.5929\n",
      "Epoch: 08/50 | Batch 16/19 | Cost: 0.5998\n",
      "Epoch: 09/50 | Batch 00/19 | Cost: 0.5727\n",
      "Epoch: 09/50 | Batch 04/19 | Cost: 0.6159\n",
      "Epoch: 09/50 | Batch 08/19 | Cost: 0.6117\n",
      "Epoch: 09/50 | Batch 12/19 | Cost: 0.6055\n",
      "Epoch: 09/50 | Batch 16/19 | Cost: 0.5807\n",
      "Epoch: 10/50 | Batch 00/19 | Cost: 0.6020\n",
      "Epoch: 10/50 | Batch 04/19 | Cost: 0.6111\n",
      "Epoch: 10/50 | Batch 08/19 | Cost: 0.5964\n",
      "Epoch: 10/50 | Batch 12/19 | Cost: 0.5729\n",
      "Epoch: 10/50 | Batch 16/19 | Cost: 0.5829\n",
      "Epoch: 11/50 | Batch 00/19 | Cost: 0.5626\n",
      "Epoch: 11/50 | Batch 04/19 | Cost: 0.5872\n",
      "Epoch: 11/50 | Batch 08/19 | Cost: 0.5951\n",
      "Epoch: 11/50 | Batch 12/19 | Cost: 0.5831\n",
      "Epoch: 11/50 | Batch 16/19 | Cost: 0.5659\n",
      "Epoch: 12/50 | Batch 00/19 | Cost: 0.5894\n",
      "Epoch: 12/50 | Batch 04/19 | Cost: 0.5677\n",
      "Epoch: 12/50 | Batch 08/19 | Cost: 0.5602\n",
      "Epoch: 12/50 | Batch 12/19 | Cost: 0.5996\n",
      "Epoch: 12/50 | Batch 16/19 | Cost: 0.5521\n",
      "Epoch: 13/50 | Batch 00/19 | Cost: 0.5728\n",
      "Epoch: 13/50 | Batch 04/19 | Cost: 0.5744\n",
      "Epoch: 13/50 | Batch 08/19 | Cost: 0.5513\n",
      "Epoch: 13/50 | Batch 12/19 | Cost: 0.5370\n",
      "Epoch: 13/50 | Batch 16/19 | Cost: 0.5588\n",
      "Epoch: 14/50 | Batch 00/19 | Cost: 0.5255\n",
      "Epoch: 14/50 | Batch 04/19 | Cost: 0.5322\n",
      "Epoch: 14/50 | Batch 08/19 | Cost: 0.5565\n",
      "Epoch: 14/50 | Batch 12/19 | Cost: 0.5511\n",
      "Epoch: 14/50 | Batch 16/19 | Cost: 0.5218\n",
      "Epoch: 15/50 | Batch 00/19 | Cost: 0.5577\n",
      "Epoch: 15/50 | Batch 04/19 | Cost: 0.5337\n",
      "Epoch: 15/50 | Batch 08/19 | Cost: 0.5172\n",
      "Epoch: 15/50 | Batch 12/19 | Cost: 0.5269\n",
      "Epoch: 15/50 | Batch 16/19 | Cost: 0.5580\n",
      "Epoch: 16/50 | Batch 00/19 | Cost: 0.5567\n",
      "Epoch: 16/50 | Batch 04/19 | Cost: 0.5184\n",
      "Epoch: 16/50 | Batch 08/19 | Cost: 0.5403\n",
      "Epoch: 16/50 | Batch 12/19 | Cost: 0.5396\n",
      "Epoch: 16/50 | Batch 16/19 | Cost: 0.5468\n",
      "Epoch: 17/50 | Batch 00/19 | Cost: 0.5270\n",
      "Epoch: 17/50 | Batch 04/19 | Cost: 0.5812\n",
      "Epoch: 17/50 | Batch 08/19 | Cost: 0.5404\n",
      "Epoch: 17/50 | Batch 12/19 | Cost: 0.5272\n",
      "Epoch: 17/50 | Batch 16/19 | Cost: 0.5449\n",
      "Epoch: 18/50 | Batch 00/19 | Cost: 0.5313\n",
      "Epoch: 18/50 | Batch 04/19 | Cost: 0.5072\n",
      "Epoch: 18/50 | Batch 08/19 | Cost: 0.5334\n",
      "Epoch: 18/50 | Batch 12/19 | Cost: 0.4746\n",
      "Epoch: 18/50 | Batch 16/19 | Cost: 0.5073\n",
      "Epoch: 19/50 | Batch 00/19 | Cost: 0.5631\n",
      "Epoch: 19/50 | Batch 04/19 | Cost: 0.5515\n",
      "Epoch: 19/50 | Batch 08/19 | Cost: 0.5577\n",
      "Epoch: 19/50 | Batch 12/19 | Cost: 0.5022\n",
      "Epoch: 19/50 | Batch 16/19 | Cost: 0.4884\n",
      "Epoch: 20/50 | Batch 00/19 | Cost: 0.4927\n",
      "Epoch: 20/50 | Batch 04/19 | Cost: 0.5614\n",
      "Epoch: 20/50 | Batch 08/19 | Cost: 0.4894\n",
      "Epoch: 20/50 | Batch 12/19 | Cost: 0.4770\n",
      "Epoch: 20/50 | Batch 16/19 | Cost: 0.4567\n",
      "Epoch: 21/50 | Batch 00/19 | Cost: 0.5235\n",
      "Epoch: 21/50 | Batch 04/19 | Cost: 0.4971\n",
      "Epoch: 21/50 | Batch 08/19 | Cost: 0.5246\n",
      "Epoch: 21/50 | Batch 12/19 | Cost: 0.5244\n",
      "Epoch: 21/50 | Batch 16/19 | Cost: 0.5279\n",
      "Epoch: 22/50 | Batch 00/19 | Cost: 0.5488\n",
      "Epoch: 22/50 | Batch 04/19 | Cost: 0.5494\n",
      "Epoch: 22/50 | Batch 08/19 | Cost: 0.5173\n",
      "Epoch: 22/50 | Batch 12/19 | Cost: 0.4595\n",
      "Epoch: 22/50 | Batch 16/19 | Cost: 0.4915\n",
      "Epoch: 23/50 | Batch 00/19 | Cost: 0.5286\n",
      "Epoch: 23/50 | Batch 04/19 | Cost: 0.4881\n",
      "Epoch: 23/50 | Batch 08/19 | Cost: 0.4503\n",
      "Epoch: 23/50 | Batch 12/19 | Cost: 0.5197\n",
      "Epoch: 23/50 | Batch 16/19 | Cost: 0.5564\n",
      "Epoch: 24/50 | Batch 00/19 | Cost: 0.4833\n",
      "Epoch: 24/50 | Batch 04/19 | Cost: 0.5454\n",
      "Epoch: 24/50 | Batch 08/19 | Cost: 0.4513\n",
      "Epoch: 24/50 | Batch 12/19 | Cost: 0.5008\n",
      "Epoch: 24/50 | Batch 16/19 | Cost: 0.4756\n",
      "Epoch: 25/50 | Batch 00/19 | Cost: 0.5239\n",
      "Epoch: 25/50 | Batch 04/19 | Cost: 0.4737\n",
      "Epoch: 25/50 | Batch 08/19 | Cost: 0.4559\n",
      "Epoch: 25/50 | Batch 12/19 | Cost: 0.5014\n",
      "Epoch: 25/50 | Batch 16/19 | Cost: 0.4921\n",
      "Epoch: 26/50 | Batch 00/19 | Cost: 0.4854\n",
      "Epoch: 26/50 | Batch 04/19 | Cost: 0.4971\n",
      "Epoch: 26/50 | Batch 08/19 | Cost: 0.5060\n",
      "Epoch: 26/50 | Batch 12/19 | Cost: 0.4836\n",
      "Epoch: 26/50 | Batch 16/19 | Cost: 0.4739\n",
      "Epoch: 27/50 | Batch 00/19 | Cost: 0.5093\n",
      "Epoch: 27/50 | Batch 04/19 | Cost: 0.4960\n",
      "Epoch: 27/50 | Batch 08/19 | Cost: 0.4744\n",
      "Epoch: 27/50 | Batch 12/19 | Cost: 0.4665\n",
      "Epoch: 27/50 | Batch 16/19 | Cost: 0.4852\n",
      "Epoch: 28/50 | Batch 00/19 | Cost: 0.4918\n",
      "Epoch: 28/50 | Batch 04/19 | Cost: 0.4538\n",
      "Epoch: 28/50 | Batch 08/19 | Cost: 0.4636\n",
      "Epoch: 28/50 | Batch 12/19 | Cost: 0.4579\n",
      "Epoch: 28/50 | Batch 16/19 | Cost: 0.5098\n",
      "Epoch: 29/50 | Batch 00/19 | Cost: 0.4605\n",
      "Epoch: 29/50 | Batch 04/19 | Cost: 0.5443\n",
      "Epoch: 29/50 | Batch 08/19 | Cost: 0.4754\n",
      "Epoch: 29/50 | Batch 12/19 | Cost: 0.4585\n",
      "Epoch: 29/50 | Batch 16/19 | Cost: 0.4963\n",
      "Epoch: 30/50 | Batch 00/19 | Cost: 0.4791\n",
      "Epoch: 30/50 | Batch 04/19 | Cost: 0.5102\n",
      "Epoch: 30/50 | Batch 08/19 | Cost: 0.4863\n",
      "Epoch: 30/50 | Batch 12/19 | Cost: 0.4887\n",
      "Epoch: 30/50 | Batch 16/19 | Cost: 0.4258\n",
      "Epoch: 31/50 | Batch 00/19 | Cost: 0.4748\n",
      "Epoch: 31/50 | Batch 04/19 | Cost: 0.4711\n",
      "Epoch: 31/50 | Batch 08/19 | Cost: 0.4336\n",
      "Epoch: 31/50 | Batch 12/19 | Cost: 0.4746\n",
      "Epoch: 31/50 | Batch 16/19 | Cost: 0.4677\n",
      "Epoch: 32/50 | Batch 00/19 | Cost: 0.4716\n",
      "Epoch: 32/50 | Batch 04/19 | Cost: 0.4835\n",
      "Epoch: 32/50 | Batch 08/19 | Cost: 0.4352\n",
      "Epoch: 32/50 | Batch 12/19 | Cost: 0.4132\n",
      "Epoch: 32/50 | Batch 16/19 | Cost: 0.4763\n",
      "Epoch: 33/50 | Batch 00/19 | Cost: 0.4567\n",
      "Epoch: 33/50 | Batch 04/19 | Cost: 0.4658\n",
      "Epoch: 33/50 | Batch 08/19 | Cost: 0.4826\n",
      "Epoch: 33/50 | Batch 12/19 | Cost: 0.4609\n",
      "Epoch: 33/50 | Batch 16/19 | Cost: 0.4496\n",
      "Epoch: 34/50 | Batch 00/19 | Cost: 0.4728\n",
      "Epoch: 34/50 | Batch 04/19 | Cost: 0.4857\n",
      "Epoch: 34/50 | Batch 08/19 | Cost: 0.4674\n",
      "Epoch: 34/50 | Batch 12/19 | Cost: 0.4709\n",
      "Epoch: 34/50 | Batch 16/19 | Cost: 0.4275\n",
      "Epoch: 35/50 | Batch 00/19 | Cost: 0.4305\n",
      "Epoch: 35/50 | Batch 04/19 | Cost: 0.4558\n",
      "Epoch: 35/50 | Batch 08/19 | Cost: 0.4682\n",
      "Epoch: 35/50 | Batch 12/19 | Cost: 0.3967\n",
      "Epoch: 35/50 | Batch 16/19 | Cost: 0.4438\n",
      "Epoch: 36/50 | Batch 00/19 | Cost: 0.4485\n",
      "Epoch: 36/50 | Batch 04/19 | Cost: 0.4422\n",
      "Epoch: 36/50 | Batch 08/19 | Cost: 0.4902\n",
      "Epoch: 36/50 | Batch 12/19 | Cost: 0.4524\n",
      "Epoch: 36/50 | Batch 16/19 | Cost: 0.4293\n",
      "Epoch: 37/50 | Batch 00/19 | Cost: 0.4410\n",
      "Epoch: 37/50 | Batch 04/19 | Cost: 0.4661\n",
      "Epoch: 37/50 | Batch 08/19 | Cost: 0.4880\n",
      "Epoch: 37/50 | Batch 12/19 | Cost: 0.3822\n",
      "Epoch: 37/50 | Batch 16/19 | Cost: 0.4813\n",
      "Epoch: 38/50 | Batch 00/19 | Cost: 0.4672\n",
      "Epoch: 38/50 | Batch 04/19 | Cost: 0.4069\n",
      "Epoch: 38/50 | Batch 08/19 | Cost: 0.4567\n",
      "Epoch: 38/50 | Batch 12/19 | Cost: 0.4023\n",
      "Epoch: 38/50 | Batch 16/19 | Cost: 0.4410\n",
      "Epoch: 39/50 | Batch 00/19 | Cost: 0.5110\n",
      "Epoch: 39/50 | Batch 04/19 | Cost: 0.4284\n",
      "Epoch: 39/50 | Batch 08/19 | Cost: 0.4154\n",
      "Epoch: 39/50 | Batch 12/19 | Cost: 0.4411\n",
      "Epoch: 39/50 | Batch 16/19 | Cost: 0.4012\n",
      "Epoch: 40/50 | Batch 00/19 | Cost: 0.4512\n",
      "Epoch: 40/50 | Batch 04/19 | Cost: 0.4080\n",
      "Epoch: 40/50 | Batch 08/19 | Cost: 0.4442\n",
      "Epoch: 40/50 | Batch 12/19 | Cost: 0.4340\n",
      "Epoch: 40/50 | Batch 16/19 | Cost: 0.4237\n",
      "Epoch: 41/50 | Batch 00/19 | Cost: 0.4308\n",
      "Epoch: 41/50 | Batch 04/19 | Cost: 0.3934\n",
      "Epoch: 41/50 | Batch 08/19 | Cost: 0.4110\n",
      "Epoch: 41/50 | Batch 12/19 | Cost: 0.3852\n",
      "Epoch: 41/50 | Batch 16/19 | Cost: 0.5147\n",
      "Epoch: 42/50 | Batch 00/19 | Cost: 0.4383\n",
      "Epoch: 42/50 | Batch 04/19 | Cost: 0.4695\n",
      "Epoch: 42/50 | Batch 08/19 | Cost: 0.3670\n",
      "Epoch: 42/50 | Batch 12/19 | Cost: 0.4203\n",
      "Epoch: 42/50 | Batch 16/19 | Cost: 0.4651\n",
      "Epoch: 43/50 | Batch 00/19 | Cost: 0.4292\n",
      "Epoch: 43/50 | Batch 04/19 | Cost: 0.4043\n",
      "Epoch: 43/50 | Batch 08/19 | Cost: 0.4036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/50 | Batch 12/19 | Cost: 0.4068\n",
      "Epoch: 43/50 | Batch 16/19 | Cost: 0.4229\n",
      "Epoch: 44/50 | Batch 00/19 | Cost: 0.4472\n",
      "Epoch: 44/50 | Batch 04/19 | Cost: 0.4091\n",
      "Epoch: 44/50 | Batch 08/19 | Cost: 0.4216\n",
      "Epoch: 44/50 | Batch 12/19 | Cost: 0.4346\n",
      "Epoch: 44/50 | Batch 16/19 | Cost: 0.3692\n",
      "Epoch: 45/50 | Batch 00/19 | Cost: 0.4206\n",
      "Epoch: 45/50 | Batch 04/19 | Cost: 0.3883\n",
      "Epoch: 45/50 | Batch 08/19 | Cost: 0.4252\n",
      "Epoch: 45/50 | Batch 12/19 | Cost: 0.4014\n",
      "Epoch: 45/50 | Batch 16/19 | Cost: 0.3760\n",
      "Epoch: 46/50 | Batch 00/19 | Cost: 0.4398\n",
      "Epoch: 46/50 | Batch 04/19 | Cost: 0.3907\n",
      "Epoch: 46/50 | Batch 08/19 | Cost: 0.4159\n",
      "Epoch: 46/50 | Batch 12/19 | Cost: 0.4326\n",
      "Epoch: 46/50 | Batch 16/19 | Cost: 0.4304\n",
      "Epoch: 47/50 | Batch 00/19 | Cost: 0.3811\n",
      "Epoch: 47/50 | Batch 04/19 | Cost: 0.4069\n",
      "Epoch: 47/50 | Batch 08/19 | Cost: 0.4429\n",
      "Epoch: 47/50 | Batch 12/19 | Cost: 0.3910\n",
      "Epoch: 47/50 | Batch 16/19 | Cost: 0.4131\n",
      "Epoch: 48/50 | Batch 00/19 | Cost: 0.4132\n",
      "Epoch: 48/50 | Batch 04/19 | Cost: 0.3461\n",
      "Epoch: 48/50 | Batch 08/19 | Cost: 0.4229\n",
      "Epoch: 48/50 | Batch 12/19 | Cost: 0.4211\n",
      "Epoch: 48/50 | Batch 16/19 | Cost: 0.3874\n",
      "Epoch: 49/50 | Batch 00/19 | Cost: 0.4149\n",
      "Epoch: 49/50 | Batch 04/19 | Cost: 0.4015\n",
      "Epoch: 49/50 | Batch 08/19 | Cost: 0.4209\n",
      "Epoch: 49/50 | Batch 12/19 | Cost: 0.4039\n",
      "Epoch: 49/50 | Batch 16/19 | Cost: 0.3824\n",
      "Epoch: 50/50 | Batch 00/19 | Cost: 0.3816\n",
      "Epoch: 50/50 | Batch 04/19 | Cost: 0.4476\n",
      "Epoch: 50/50 | Batch 08/19 | Cost: 0.4234\n",
      "Epoch: 50/50 | Batch 12/19 | Cost: 0.4800\n",
      "Epoch: 50/50 | Batch 16/19 | Cost: 0.4045\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # set the model into train mode\n",
    "    model.train()\n",
    "    \n",
    "    for batch_index, (features, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # do one forward pass\n",
    "        predicted_probabilities = model(features)\n",
    "        \n",
    "        # y_preds.squeeze() removes the matrix dimension (ie. 3 x 1) into just an array of length 3\n",
    "        loss = loss_criterion(predicted_probabilities.squeeze(), labels)\n",
    "        \n",
    "        # use adam backward solver\n",
    "        optimizer.zero_grad() # don't make the computation graph, we dont need since we throw away after\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_index % 4:\n",
    "            print('Epoch: %02d/%02d | Batch %02d/%02d | Cost: %.4f'  %(epoch+1, num_epochs, batch_index, len(train_loader), loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f74057d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back in test mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd2b6f",
   "metadata": {},
   "source": [
    "## Evaluate the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3e4ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for features, labels in test_loader:\n",
    "        \n",
    "        logits = model(features)\n",
    "        y_preds = (logits.squeeze() > 0.5).float() # squeeze creates a tensor of True/False values\n",
    "                                                        # which are evaluated to 1.0 and 0.0\n",
    "            \n",
    "        total += labels.size(0)\n",
    "        correct += (y_preds == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "78b9aed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.49122807017544"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 100* correct/total\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8f312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
